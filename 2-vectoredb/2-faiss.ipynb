{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90a7036e",
   "metadata": {},
   "source": [
    "### Building a RAG System with LangChain and FAISS \n",
    "Introduction to RAG (Retrieval-Augmented Generation)\n",
    "RAG combines the power of retrieval systems with generative AI models. Instead of relying solely on the model's training data, RAG:\n",
    "\n",
    "1. Retrieves relevant documents from a knowledge base\n",
    "2. Uses these documents as context for the LLM\n",
    "3. Generates responses based on both the retrieved context and the model's knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e937ab9f",
   "metadata": {},
   "source": [
    "### FAISS \n",
    "https://github.com/facebookresearch/faiss\n",
    "\n",
    "FAISS is a library for efficient similarity search and clustering of dense vectors.\n",
    "\n",
    "Key advantages:\n",
    "1. Extremely fast similarity search\n",
    "2. Memory efficient\n",
    "3. Supports GPU acceleration\n",
    "4. Can handle millions of vectors\n",
    "\n",
    "How it works:\n",
    "- Indexes vectors for fast nearest neighbor search\n",
    "- Returns most similar vectors based on distance metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain core imports\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import (\n",
    "    RunnablePassthrough, \n",
    " \n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# LangChain specific imports\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4adfdc",
   "metadata": {},
   "source": [
    "### Data Ingestion And Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511190a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_documents = [\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Artificial Intelligence (AI) is the simulation of human intelligence in machines.\n",
    "        These systems are designed to think like humans and mimic their actions.\n",
    "        AI can be categorized into narrow AI and general AI.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"AI Introduction\", \"page\": 1, \"topic\": \"AI\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Machine Learning is a subset of AI that enables systems to learn from data.\n",
    "        Instead of being explicitly programmed, ML algorithms find patterns in data.\n",
    "        Common types include supervised, unsupervised, and reinforcement learning.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"ML Basics\", \"page\": 1, \"topic\": \"ML\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Deep Learning is a subset of machine learning based on artificial neural networks.\n",
    "        It uses multiple layers to progressively extract higher-level features from raw input.\n",
    "        Deep learning has revolutionized computer vision, NLP, and speech recognition.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"Deep Learning\", \"page\": 1, \"topic\": \"DL\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Natural Language Processing (NLP) is a branch of AI that helps computers understand human language.\n",
    "        It combines computational linguistics with machine learning and deep learning models.\n",
    "        Applications include chatbots, translation, sentiment analysis, and text summarization.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"NLP Overview\", \"page\": 1, \"topic\": \"NLP\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "print(sample_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028d3f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## text splitting\n",
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\" \"]\n",
    ")\n",
    "\n",
    "## split the documents into chunks\n",
    "chunks = text_splitter.split_documents(sample_documents)\n",
    "print(chunks[0])\n",
    "print(chunks[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed228b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Created {len(chunks)} chunks from {len(sample_documents)} documents\")\n",
    "print(\"\\nExample chunk:\")\n",
    "print(f\"Content: {chunks[0].page_content}\")\n",
    "print(f\"Metadata: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1051ca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the embedding models\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab0c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI embeddings with the latest model\n",
    "\n",
    "embeddings=OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    dimensions=1536\n",
    ")\n",
    "\n",
    "## Example: create a embedding for a single text\n",
    "sample_text=\"What is machine learning\"\n",
    "sample_embedding=embeddings.embed_query(sample_text)\n",
    "sample_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c19c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[\"AI\",\"MAchine learning\",\"Deep Learning\",\"Neural Network\"]\n",
    "batch_embeddings=embeddings.embed_documents(texts)\n",
    "print(batch_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce23d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4452d27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compare Embedding using cosine similarity\n",
    "\n",
    "def compare_embeddings(text1:str,text2:str):\n",
    "    \"\"\"Compare semantic simialrity of 2 texts usign embeddings\"\"\"\n",
    "\n",
    "    emb1=np.array(embeddings.embed_query(text1))\n",
    "    emb2=np.array(embeddings.embed_query(text2))\n",
    "\n",
    "    ## Calculate the simialrity score\n",
    "\n",
    "    similarity=np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d2e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic similarity\n",
    "print(\"\\nSemantic Similarity Examples:\")\n",
    "print(f\"'AI' vs 'Artificial Intelligence': {compare_embeddings('AI', 'Artificial Intelligence'):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ea6092",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"'AI' vs 'Pizza': {compare_embeddings('AI', 'Pizza'):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50296f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"'Machine Learning' vs 'ML': {compare_embeddings('Machine Learning', 'ML'):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9735ac2",
   "metadata": {},
   "source": [
    "### Create FAISS Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0989a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore=FAISS.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings\n",
    ")\n",
    "print(f\"Vector store created with {vectorstore.index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d15527",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d43809",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save vector tore for later use\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "print(\"Vector store saved to 'faiss_index' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd954a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load vector store\n",
    "loaded_vectorstore=FAISS.load_local(\n",
    "    \"faiss_index\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "print(f\"Loaded vector store contains {loaded_vectorstore.index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca90941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Similarity Search \n",
    "query=\"What is deep learning\"\n",
    "\n",
    "results=vectorstore.similarity_search(query,k=3)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b21fff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top 3 similar chunks:\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n{i+1}. Source: {doc.metadata['source']}\")\n",
    "    print(f\"   Content: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0234314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Similarity Search with score\n",
    "results_with_scores=vectorstore.similarity_search_with_score(query,k=3)\n",
    "\n",
    "print(\"\\n\\nSimilarity search with scores:\")\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"\\nScore: {score:.3f}\")\n",
    "    print(f\"Source: {doc.metadata['source']}\")\n",
    "    print(f\"Content preview: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a80f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e0d681",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search with metadata filtering\n",
    "filter_dict={\"topic\":\"ML\"}\n",
    "filtered_results=vectorstore.similarity_search(\n",
    "    query,\n",
    "    k=3,\n",
    "    filter=filter_dict\n",
    ")\n",
    "print(filtered_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f153009",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3214a1d",
   "metadata": {},
   "source": [
    "### Build RAG Chain With LCEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6de908",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LLM GROQ LLM\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=init_chat_model(model=\"groq:gemma2-9b-it\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c28e9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26844d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Simple RAG Chain with LCEL\n",
    "simple_prompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based only on the following context:\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f83433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic retriever\n",
    "retriever=vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a9fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3db5d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "# Format documents for the prompt\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    \"\"\"Format documents for insertion into prompt\"\"\"\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        formatted.append(f\"Document {i+1} (Source: {source}):\\n{doc.page_content}\")\n",
    "    return \"\\n\\n\".join(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6f65ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_chain=(\n",
    "    {\"context\":retriever | format_docs,\"question\":RunnablePassthrough() }\n",
    "    | simple_prompt\n",
    "    | llm\n",
    "    |StrOutputParser()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c470a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18098855",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Conversational RAg Chain\n",
    "\n",
    "conversational_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Use the provided context to answer questions.\"),\n",
    "    (\"placeholder\", \"{chat_history}\"),\n",
    "    (\"human\", \"Context: {context}\\n\\nQuestion: {input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42689e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversational_rag():\n",
    "    \"\"\"Create a conversational RAG chain with memory\"\"\"\n",
    "    return (\n",
    "        RunnablePassthrough.assign(\n",
    "            context=lambda x: format_docs(retriever.invoke(x[\"input\"]))\n",
    "        )\n",
    "        | conversational_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "conversational_rag = create_conversational_rag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f255f6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88574dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### streaming RAG chain\n",
    "streaming_rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | simple_prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "print(\"Modern RAG chains created successfully!\")\n",
    "print(\"Available chains:\")\n",
    "print(\"- simple_rag_chain: Basic Q&A\")\n",
    "print(\"- conversational_rag: Maintains conversation history\")\n",
    "print(\"- streaming_rag_chain: Supports token streaming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8a26f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function for different chain types\n",
    "def test_rag_chains(question: str):\n",
    "    \"\"\"Test all RAG chain variants\"\"\"\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Simple RAG\n",
    "    print(\"\\n1. Simple RAG Chain:\")\n",
    "    answer = simple_rag_chain.invoke(question)\n",
    "    print(f\"Answer: {answer}\")\n",
    "\n",
    "    print(\"\\n2. Streaming RAG:\")\n",
    "    print(\"Answer: \", end=\"\", flush=True)\n",
    "    for chunk in streaming_rag_chain.stream(question):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a9cd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rag_chains(\"What is the difference between AI and machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e375d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with multiple questions\n",
    "test_questions = [\n",
    "    \"What is the difference between AI and Machine Learning?\",\n",
    "    \"Explain deep learning in simple terms\",\n",
    "    \"How does NLP work?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "    test_rag_chains(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcfe3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conversational example\n",
    "print(\"\\n3. Conversational RAG Example:\")\n",
    "chat_history = []\n",
    "\n",
    "# First question\n",
    "q1 = \"What is machine learning?\"\n",
    "a1 = conversational_rag.invoke({\n",
    "    \"input\": q1,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "\n",
    "print(f\"Q1: {q1}\")\n",
    "print(f\"A1: {a1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a72dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update history\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=q1),\n",
    "    AIMessage(content=a1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9eda39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up question\n",
    "q2 = \"How is it different from traditional programming?\"\n",
    "a2 = conversational_rag.invoke({\n",
    "    \"input\": q2,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "print(f\"\\nQ2: {q2}\")\n",
    "print(f\"A2: {a2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ada94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
